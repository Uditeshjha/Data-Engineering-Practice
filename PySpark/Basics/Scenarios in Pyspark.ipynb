{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"day5\").getOrCreate()"
      ],
      "metadata": {
        "id": "Uw91kQfZEt55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find origin and destination for customers and their total amount"
      ],
      "metadata": {
        "id": "_TS015yJFfFe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_qS-rYmDDba",
        "outputId": "40bb0f44-339a-4bb2-9d40-4d3c93fc8a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+-----------+-----+\n",
            "|cust_id|flight_id|   origin|destination|price|\n",
            "+-------+---------+---------+-----------+-----+\n",
            "|      1|       f3|    kochi|  Mangalore| 1800|\n",
            "|      1|       f1|    delhi|  hyderabad| 2500|\n",
            "|      2|       f2|  Ayodhya|    chennai| 3000|\n",
            "|      1|       f2|hyderabad|      kochi| 1700|\n",
            "|      2|       f1|   Mumbai|    Ayodhya| 4000|\n",
            "+-------+---------+---------+-----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_travel_csv = spark.read.csv('travel.csv',header=True)\n",
        "df_travel_csv.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "Xuo5P4cbHkK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_travel_results = df_travel_csv.orderBy('cust_id','flight_id').groupBy('cust_id').agg(first('origin')\\\n",
        "                                                    .alias('orogin'),last('destination').alias('destination')\\\n",
        "                                                    ,sum('price').alias('total_amount'))\n",
        "df_travel_results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2m5Ua2TEs9b",
        "outputId": "91874182-0cc3-41be-aac0-ecaaf587ccc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------+------------+\n",
            "|cust_id|orogin|destination|total_amount|\n",
            "+-------+------+-----------+------------+\n",
            "|      1| delhi|  Mangalore|      6000.0|\n",
            "|      2|Mumbai|    chennai|      7000.0|\n",
            "+-------+------+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert the given i/p table to given o/p table<br>\n",
        "\n",
        "I/P:  \n",
        "```\n",
        "+------+------+------+  \n",
        "|Team_1|Team_2|Winner|  \n",
        "+------+------+------+  \n",
        "| India|    SL| India|  \n",
        "|    SL|   Aus|   Aus|  \n",
        "|    SA|   Eng|   Eng|  \n",
        "|   Eng|    NZ|    NZ|  \n",
        "|   Aus| India| India|  \n",
        "+------+------+------+  \n",
        "```\n",
        "\n",
        "O/P:  \n",
        "```\n",
        "+---------+--------------+----------+-----------+\n",
        "|Team_name|matches_played|No_of_wins|No_of_loses|\n",
        "+---------+--------------+----------+-----------+\n",
        "|       SL|             2|         0|          2|\n",
        "|    India|             2|         2|          0|\n",
        "|      Eng|             2|         1|          1|\n",
        "|       SA|             1|         0|          1|\n",
        "|      Aus|             2|         1|          1|\n",
        "|       NZ|             1|         1|          0|\n",
        "+---------+--------------+----------+-----------+\n",
        "```"
      ],
      "metadata": {
        "id": "5iQ6ktkgJRU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    (\"India\", \"SL\", \"India\"),\n",
        "    (\"SL\", \"Aus\", \"Aus\"),\n",
        "    (\"SA\", \"Eng\", \"Eng\"),\n",
        "    (\"Eng\", \"NZ\", \"NZ\"),\n",
        "    (\"Aus\", \"India\", \"India\"),\n",
        "]\n",
        "\n",
        "columns = [\"Team_1\", \"Team_2\", \"Winner\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpVsApY4HHj8",
        "outputId": "5db91692-a030-48ce-bc64-0317ea5a3e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+\n",
            "|Team_1|Team_2|Winner|\n",
            "+------+------+------+\n",
            "| India|    SL| India|\n",
            "|    SL|   Aus|   Aus|\n",
            "|    SA|   Eng|   Eng|\n",
            "|   Eng|    NZ|    NZ|\n",
            "|   Aus| India| India|\n",
            "+------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Union All -> gives all data, keep duplicate, no sorting, faster\n",
        "# Union -> gives unique data, sorting is done, slow\n",
        "\n",
        "df_col1 = df.select('Team_1')\n",
        "df_col2 = df.select('Team_2')\n",
        "# union_teams_df = df_col1.union(df_col2).distinct().show()\n",
        "\n",
        "# Derived column -> matches played\n",
        "union_teams_df = df_col1.union(df_col2).groupBy('Team_1').agg(count('Team_1').alias('matches_played'))\\\n",
        "                  .withColumnRenamed('Team_1','Team_name')\n",
        "union_teams_df.show()\n",
        "\n",
        "# Winner column\n",
        "join_df = union_teams_df.join(df,union_teams_df.Team_name == df.Winner,'left').drop('Team_1','Team_2')\n",
        "# join_df.show()\n",
        "\n",
        "# Derived column -> No. of wins\n",
        "join_df = join_df.groupBy('Team_name','matches_played').agg(count('Winner').alias('No_of_wins'))\n",
        "# join_df.show()\n",
        "\n",
        "# Derived column -> No. of loses\n",
        "#join_df.printSchema()\n",
        "final_result_df = join_df.withColumn('No_of_loses',col('matches_played') - col('No_of_wins'))\n",
        "final_result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz757TdNK2-U",
        "outputId": "80ae057f-a022-46b0-f8e9-c18eee9d864d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+\n",
            "|Team_name|matches_played|\n",
            "+---------+--------------+\n",
            "|       SL|             2|\n",
            "|    India|             2|\n",
            "|      Eng|             2|\n",
            "|       SA|             1|\n",
            "|      Aus|             2|\n",
            "|       NZ|             1|\n",
            "+---------+--------------+\n",
            "\n",
            "+---------+--------------+------+\n",
            "|Team_name|matches_played|Winner|\n",
            "+---------+--------------+------+\n",
            "|       SL|             2|  NULL|\n",
            "|    India|             2| India|\n",
            "|    India|             2| India|\n",
            "|      Eng|             2|   Eng|\n",
            "|       SA|             1|  NULL|\n",
            "|      Aus|             2|   Aus|\n",
            "|       NZ|             1|    NZ|\n",
            "+---------+--------------+------+\n",
            "\n",
            "+---------+--------------+----------+\n",
            "|Team_name|matches_played|No_of_wins|\n",
            "+---------+--------------+----------+\n",
            "|       SL|             2|         0|\n",
            "|    India|             2|         2|\n",
            "|      Eng|             2|         1|\n",
            "|       SA|             1|         0|\n",
            "|      Aus|             2|         1|\n",
            "|       NZ|             1|         1|\n",
            "+---------+--------------+----------+\n",
            "\n",
            "+---------+--------------+----------+-----------+\n",
            "|Team_name|matches_played|No_of_wins|No_of_loses|\n",
            "+---------+--------------+----------+-----------+\n",
            "|       SL|             2|         0|          2|\n",
            "|    India|             2|         2|          0|\n",
            "|      Eng|             2|         1|          1|\n",
            "|       SA|             1|         0|          1|\n",
            "|      Aus|             2|         1|          1|\n",
            "|       NZ|             1|         1|          0|\n",
            "+---------+--------------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a PySpark program to select each employee's primary department.\n",
        "\n",
        "### If primary_flag = 'Y' exists, pick that, otherwise pick any/default department.\n",
        "\n",
        "I/P:\n",
        "\n",
        "```\n",
        "employee_id | department_id | primary_flag\n",
        "-----------------------------------------\n",
        "1           | 1             | N\n",
        "2           | 1             | Y\n",
        "2           | 3             | N\n",
        "3           | 3             | N\n",
        "4           | 2             | Y\n",
        "4           | 3             | N\n",
        "```"
      ],
      "metadata": {
        "id": "YQ7FRszcRNK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
      ],
      "metadata": {
        "id": "y41tbLNISKby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"employee_id\", IntegerType(), True),\n",
        "    StructField(\"department_id\", IntegerType(), True),\n",
        "    StructField(\"primary_flag\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Input data\n",
        "data = [\n",
        "    (1, 1, \"N\"),\n",
        "    (2, 1, \"Y\"),\n",
        "    (2, 3, \"N\"),\n",
        "    (3, 3, \"N\"),\n",
        "    (4, 2, \"N\"),\n",
        "    (4, 3, \"N\"),\n",
        "    (4, 4, \"Y\")\n",
        "]\n",
        "\n",
        "df_emp = spark.createDataFrame(data, schema)\n",
        "\n",
        "df_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAdjCWjjL1BA",
        "outputId": "a9f834b9-afb8-4f88-b185-fd5fdc0371e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+------------+\n",
            "|employee_id|department_id|primary_flag|\n",
            "+-----------+-------------+------------+\n",
            "|          1|            1|           N|\n",
            "|          2|            1|           Y|\n",
            "|          2|            3|           N|\n",
            "|          3|            3|           N|\n",
            "|          4|            2|           N|\n",
            "|          4|            3|           N|\n",
            "|          4|            4|           Y|\n",
            "+-----------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "oGT3upa1Sm4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "W = Window.partitionBy('employee_id').orderBy(col('primary_flag').desc())\n",
        "df_emp_result = df_emp.withColumn('rn',row_number().over(W))\\\n",
        "                                  .filter('rn = 1')\\\n",
        "                                  .select(\"employee_id\", \"department_id\")\n",
        "df_emp_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjHYjIuwSTJp",
        "outputId": "b3c6e7f8-d8ef-4795-c00f-c43bf6238868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|employee_id|department_id|\n",
            "+-----------+-------------+\n",
            "|          1|            1|\n",
            "|          2|            1|\n",
            "|          3|            3|\n",
            "|          4|            4|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2"
      ],
      "metadata": {
        "id": "_3sZ3tvRmD7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Assignment2\").getOrCreate()\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField(\"customer_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Sample input data\n",
        "data = [\n",
        "    (\"kasireddy naidu\",),\n",
        "    (\"konidela ram charan\",),\n",
        "    (\"Nandamuri tarak ramarao\",),\n",
        "    (\"charan\",)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "aLupMFJumPaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06fe05bf-787f-471d-f9d9-9e5734879fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+\n",
            "|customer_name          |\n",
            "+-----------------------+\n",
            "|kasireddy naidu        |\n",
            "|konidela ram charan    |\n",
            "|Nandamuri tarak ramarao|\n",
            "|charan                 |\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, size, col, when\n",
        "\n",
        "# Split name into array\n",
        "df_split = df.withColumn(\"name_arr\", split(col(\"customer_name\"), \" \"))\n",
        "\n",
        "df_final = df_split.select(\n",
        "    col(\"customer_name\"),\n",
        "\n",
        "    when(size(col(\"name_arr\")) >= 1, col(\"name_arr\")[0])\n",
        "        .otherwise(None).alias(\"first_name\"),\n",
        "\n",
        "    when(size(col(\"name_arr\")) >= 3, col(\"name_arr\")[1])\n",
        "        .when(size(col(\"name_arr\")) == 2, None)     # 2-word name → no middle name\n",
        "        .otherwise(None).alias(\"middle_name\"),\n",
        "\n",
        "    when(size(col(\"name_arr\")) >= 2, col(\"name_arr\")[size(col(\"name_arr\")) - 1])\n",
        "        .otherwise(col(\"name_arr\")[0]).alias(\"last_name\")\n",
        ")\n",
        "\n",
        "df_final.show(truncate=False)"
      ],
      "metadata": {
        "id": "cHIa9ymfTBCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8011f03f-52d8-44f3-9a04-3e91300179f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+----------+-----------+---------+\n",
            "|customer_name          |first_name|middle_name|last_name|\n",
            "+-----------------------+----------+-----------+---------+\n",
            "|kasireddy naidu        |kasireddy |NULL       |naidu    |\n",
            "|konidela ram charan    |konidela  |ram        |charan   |\n",
            "|Nandamuri tarak ramarao|Nandamuri |tarak      |ramarao  |\n",
            "|charan                 |charan    |NULL       |charan   |\n",
            "+-----------------------+----------+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 3"
      ],
      "metadata": {
        "id": "_G_4pFZiooF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "park = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, 101, 500.0, \"2024-01-01\"),\n",
        "    (2, 101, 600.0, \"2024-01-01\"),\n",
        "    (3, 101, 200.0, \"2024-01-02\"),\n",
        "    (4, 101, 300.0, \"2024-01-03\"),\n",
        "    (5, 102, 400.0, \"2024-01-05\"),\n",
        "    (6, 103, 600.0, \"2024-01-06\"),\n",
        "    (7, 101, 200.0, \"2024-01-07\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"t_id\", \"user_id\", \"amount\", \"t_date\"])\n",
        "\n",
        "# count orders per user\n",
        "# user_order_counts = df.groupBy(\"user_id\") \\\n",
        "#                       .agg(count(\"*\").alias(\"order_count\"))\n",
        "\n",
        "# # filter users with minimum 3 orders\n",
        "# eligible_users = user_order_counts.filter(col(\"order_count\") >= 3)\n",
        "\n",
        "# # join back and compute total spending\n",
        "# result = df.join(eligible_users, \"user_id\", \"inner\") \\\n",
        "#            .groupBy(\"user_id\") \\\n",
        "#            .agg(sum_(\"amount\").alias(\"total_spend\"))\n",
        "\n",
        "# result.show()\n",
        "\n",
        "\n",
        "result = (\n",
        "    df.groupBy(\"user_id\")\n",
        "      .agg(\n",
        "          count(\"*\").alias(\"order_count\"),\n",
        "          sum(\"amount\").alias(\"total_spend\")\n",
        "      )\n",
        "      .filter(col(\"order_count\") >= 3)\n",
        "      .select(\"user_id\", \"total_spend\")\n",
        ")\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "id": "yp_pt6sFmRJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0634d1c9-074c-4743-ad46-f5896a7b3855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|user_id|total_spend|\n",
            "+-------+-----------+\n",
            "|    101|     1800.0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 4"
      ],
      "metadata": {
        "id": "_LdaSS6VpGrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"RCB\",),\n",
        "    (\"CSK\",),\n",
        "    (\"MI\",),\n",
        "    (\"PBKS\",)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"team\"])\n",
        "df.show()\n",
        "\n",
        "df_matches = (\n",
        "    df.alias(\"a\")\n",
        "      .crossJoin(df.alias(\"b\"))\n",
        "      .filter(col(\"a.team\") < col(\"b.team\"))   # ensures A<B → no repetition\n",
        "      .select(\n",
        "          col(\"a.team\").alias(\"team1\"),\n",
        "          col(\"b.team\").alias(\"team2\")\n",
        "      )\n",
        ")\n",
        "\n",
        "df_matches.show()"
      ],
      "metadata": {
        "id": "TltuWhz3p4nU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900e78d4-9ce6-422f-9227-d9186b113e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|team|\n",
            "+----+\n",
            "| RCB|\n",
            "| CSK|\n",
            "|  MI|\n",
            "|PBKS|\n",
            "+----+\n",
            "\n",
            "+-----+-----+\n",
            "|team1|team2|\n",
            "+-----+-----+\n",
            "|  CSK|  RCB|\n",
            "|  CSK|   MI|\n",
            "|  CSK| PBKS|\n",
            "|   MI|  RCB|\n",
            "| PBKS|  RCB|\n",
            "|   MI| PBKS|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6AVjy2zcIeA1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}